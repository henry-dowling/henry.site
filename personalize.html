<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>What Happens When Models Stop Getting Smarter? – Henry Dowling</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" type="image/jpeg" href="favicon.jpg">
  <style>
    body {
      line-height: 1.4;
      font-size: 16px;
      padding: 0 10px;
      margin: 50px auto;
      max-width: 650px;
    }
    #maincontent {
      max-width: 42em;
      margin: 15 auto;
    }
    details {
      margin: 1em 0;
    }
    summary {
      font-weight: 700;
      font-size: 1.17em; /* similar to h3 */
      cursor: pointer;
    }
  </style>
  </head>
  <body>
    <header>
      <h2><a href="index.html" style="color: inherit; text-decoration: none;">Henry Dowling</a>'s <a href="blog.html">Blog</a></h2>
    </header>
    <main>
      <h2>What happens when models stop getting smarter?</h2>

      <p> Here's a general rule for AI products: </p>
      <ul>
        <li> <strong>AI Response Quality = Intelligence + Context Quality. </strong> </li>
      </ul>
      <p>So, what happens if LLMs stop getting smarter? The way to win at response quality will be to have the highest <em>context quality</em>. The current battle for the most <em>intelligent</em> LLM will evolve into a battle for the <em>highest quality</em> LLM; i.e. the LLM with the best intelligence + context quality.</p>
      <p>AI products downstream of OpenAI already know this— when everyone has the same intelligence, the battle is about who can do the best <a href="https://blog.langchain.com/the-rise-of-context-engineering/">context engineering</a>. But mainstream consumer AI companies (eg ChatGPT) have focused their investment on maxing out intelligence first.</p>

      <h3>Improving context quality would be a really good thing for consumer AI</h3>


      <p>Most AI interactions today start with you explaining everything. This is annoying, especially if you've already explained everything somewhere else.</p>
      <p>This information usually isn’t a secret. What model car you drive, the link to the repo you’re working on, the names of your friends: all this info is easy to find from your search history, email, personal notes, etc. </p> 
      <p>AI interactions would be so much easier if every prompt you wrote was hydrated with context from these (easily accessible) sources. But that's not how the way things work right now.</p>

      <h3>AI's Google Ads moment</h3>

      <p>Here's what the future will look like: each person will have a consolidated memory store that consolidates facts worth remembering about the user from their online activity.</p>
      <p>This memory store will act as context-injecting middleware, ensuring that every message sent between human and AI will have extremely high <em> context quality</em>. This will lead to much more relevant, and ultimately magical AI interactions.</p>

      <p><u>This is basically exactly that happened with personalized advertising in the 2010s</u>. Last decade, we built out massive infrastructure to enable personalized advertising that pulls information from every corner of a user’s online footprint.<sup><a href="#fn2" id="fnref2">[1]</a></sup> An industrial-scale AI memory infrastructure buildout in the service of <em>context quality</em> feels inevitable.</p> 

      <h3>Model providers probably won't own the memory store</h3>
      <p>Who has access to the personal context dataset? Not model providers, for the most part. It’s highly balkanized:</p>
      <ul>
        <li>notes you scribble to yourself</li>
        <li>browsing and search history</li>
        <li>content you watch on tiktok and other platforms</li>
        <li>discord servers and group chats</li>
        <li>calendars, emails, files, code repos, etc</li>
      </ul>

      <p>A new company, sitting on some store of high-quality personal context data, will have to create this.</p>

      <h3>Who’s building this?</h3>
      <p>Me and <a href="https://x.com/samzliu">Sam Liu</a>. We’re building <a href="https://allegory.to/">Allegory</a>, a notes app that lets AI take full advantage of the rich and high-quality context that you generate when you write notes to yourself to improve AI interactions. If you think this sounds cool, <a href="https://calendly.com/htdowling/coffee">say hi to us</a>.</p>
      <br>

      <hr>


      <section id="footnotes">
        <ol>
          <li id="fn2">Aside: we expected this massive consolidation of information by a few key players (we called it “Big Data”) to fundamentally change the way society functioned, but it turned out to mostly just enable better ads. <a href="#fnref1" aria-label="Back to content">↩</a></li>
        </ol>
      </section>

      <br>
      <br>

      <details>
        <summary>Postscript: What if models <em>do</em> keep getting smarter?</summary>
        <p>These trends actually pretty much hold even in the world where AI does get a lot smarter. (I opted to focus on the "what if ai not smarter" case for clickbait, haha). Let's examine the priciples underlying the arguments made in this post and show that they still apply in the world where AI gets much smarter. </p>
        <ul>
          <li><strong>Response Quality = Intelligence + Context Quality.</strong> 
            <ul>
              <li>This will still hold— it's a fact about any AI product regardless of intelligence. Incremental gains will be achievable by increasing Intelligence <em>or</em> context quality. </li>
              <li>In fact, the relationship between Intelligence and Context Quality is likely <strong>convex</strong>; i.e. the smarter a model is, the <em>more productive</em> an improvement to Context Quality is. </li>
            </ul>
          </li>
          <li><strong>The most important personal context lies outside of intelligence providers.</strong>
            <ul>
              <li> This is clearly still true; you could argue as AI intelligence improves it gives the winning player a chance at becoming the "front door to the internet", but Google already is yet hasn't devoted any serious effort to this problem since their investment has been in intelligence-maxxing.  </li>
            </ul>
          </li>
          <li><strong>AI model costs will go down.</strong>
            <ul>
              <li>
                This may not hold true for all applications, but I suspect it *will* for consumer AI. 
              </li>
              <li>
                We need to ask ourselves: is intelligence the bottleneck for any consumer AI use cases currently?
              </li>
              <li>
                For the largest consumer AI use cases (search, AI companion, cheating on homework), AI already works great. In fact, people complained when OpenAI <em>upgraded</em> ChatGPT to use more intelligent GPT-5 because it had fewer of the sycophantic qualities of 4o.
              </li>
            </ul>
          </li>
          
        </ul>
      </details>

      <details>
        <summary>Postscript: Beyond on-demand chat</summary>
        <p>In five years, we'll think it's crazy that we used to have to start every AI interaction by explaining everything. In ten years, we'll think it's crazy that we ever had to "prompt" AI <em>at all</em>. With high-enough-quality context, AI should be able to answer your question before you even ask it. Like personalized ads today, in the future AI will be able to <u>read your mind</u>.</p>
        <ul>
          <li>If it can see you’re stuck on a bug, it texts you what you’re missing.</li>
          <li>If you’re booking a vacation, it does deep research in the background to pre-empt your questions.</li>
        </ul>
        <p>As compute costs decrease (if models can’t get more intelligent, then costs have to go down), we’ll be able to invest more in <a href="https://arxiv.org/abs/2504.13171">Sleep Time Compute</a> to create these pre-empting, magical interactions.</p>
      </details>

      

    </main>
  </body>
  </html>


