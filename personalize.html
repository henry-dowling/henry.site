<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>What Happens When Models Stop Getting Smarter? – Henry Dowling</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" type="image/jpeg" href="favicon.jpg">
  <style>
    body {
      line-height: 1.4;
      font-size: 16px;
      padding: 0 10px;
      margin: 50px auto;
      max-width: 650px;
    }
    #maincontent {
      max-width: 42em;
      margin: 15 auto;
    }
  </style>
  </head>
  <body>
    <header>
      <h2><a href="index.html" style="color: inherit; text-decoration: none;">Henry Dowling</a>'s <a href="blog.html">Blog</a></h2>
    </header>
    <main>
      <h2>What happens when models stop getting smarter?</h2>

      <p>Most AI interactions today start with you explaining everything.<sup><a href="#fn1" id="fnref1">[1]</a></sup> This is annoying, especially if you've already explained everything somewhere else.</p>
      <p>This information usually isn’t a secret. What model car you drive, the link to the repo you’re working on, the names of your friends: all this info is easy to find from things you've written:search history, email, personal notes, etc. </p> 
      <p>Life would be so much easier if AI interactions could come pre-loaded with context from these (easily accessible) sources. But that's not how the way things work right now.</p>

      <h3>Model Response Quality = Intelligence + Context Quality.</h3>
      <p>So, what happens if LLMs stop getting smarter? The way to win at response quality will be to have the highest <em>context quality</em>. The current battle for the most <em>intelligent</em> LLM will evolve into a battle for the <em>highest quality</em> LLM; i.e. the LLM with the best intelligence + context quality.</p>


      <h3>Who controls the personal context?</h3>
      <p>Who has access to the personal context dataset? It’s highly balkanized:</p>
      <ul>
        <li>notes you scribble to yourself</li>
        <li>browsing and search history</li>
        <li>content you watch on tiktok and other platforms</li>
        <li>discord servers and group chats</li>
        <li>calendars, emails, files, code repos, etc</li>
      </ul>

      <h3>We’ve seen this movie before</h3>
      <p>AI interactions will personalize at an industrial scale in the 2020s, similarly to content + ads in the 2010s.</p>
      <p>The last major AI wave—personalized recommendation algorithms—was mostly about finding context on someone and using it to generate high-quality outputs.<sup><a href="#fn2" id="fnref2">[2]</a></sup></p>
      <p>Last decade, we built out massive infrastructure to enable personalized advertising that pulls information from every corner of a user’s online footprint. We will need to execute a similar-scale infrastructure buildout in order to win the war for best context quality.</p>

      <h3>Beyond on-demand chat</h3>
      <p>With enough context, AI will stop being an on-demand chat product. Right now, in order to get value out of AI, you need to prompt it. With sufficient context, AI should be able to answer your question before you even ask it:</p>
      <ul>
        <li>If it can see you’re stuck on a bug, it texts you what you’re missing.</li>
        <li>If you’re booking a vacation, it does deep research in the background to pre-empt your questions.</li>
      </ul>
      <p>As compute costs decrease (if models can’t get more intelligent, then costs have to go down), we’ll be able to invest more in <a href= https://arxiv.org/abs/2504.13171>Sleep Time Compute</a> to create these pre-empting, magical interactions.</p>

      <h3>Who’s building this?</h3>
      <p>Me and <a href= https://x.com/samzliu> Sam Liu</a>. We’re building <a href="https://allegory.to/">Allegory</a>, a notes app that lets AI take full advantage of the rich and high-quality context that you generate when you write notes to yourself to improve AI interactions. If you think this sounds cool, <a href= https://calendly.com/htdowling/coffee </a> say hi to us</a>.</p>

      <hr>
      <section id="footnotes">
        <ol>
          <li id="fn1">Paraphrasing <a href=https://www.anthropic.com/news/connectors-directory>Anthropic</a>, who articulated it really well. <a href="#fnref3" aria-label="Back to content">↩</a></li>
          <li id="fn2">Aside: we expected this massive consolidation of information by a few key players (we called it “Big Data”) to fundamentally change the way society functioned, but it turned out to mostly just enable better ads. <a href="#fnref1" aria-label="Back to content">↩</a></li>
        </ol>
      </section>

    </main>
  </body>
  </html>


