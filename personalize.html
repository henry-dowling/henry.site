<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>What Happens When Models Stop Getting Smarter? – Henry Dowling</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" type="image/jpeg" href="favicon.jpg">
  <style>
    body {
      line-height: 1.4;
      font-size: 16px;
      padding: 0 10px;
      margin: 50px auto;
      max-width: 650px;
    }
    #maincontent {
      max-width: 42em;
      margin: 15 auto;
    }
  </style>
  </head>
  <body>
    <header>
      <h2><a href="index.html" style="color: inherit; text-decoration: none;">Henry Dowling</a>'s <a href="blog.html">Blog</a></h2>
    </header>
    <main>
      <h2>What happens when models stop getting smarter?</h2>

      <p> Here is a general rule for AI products: </p>
      <ul>
        <li> <strong>Response Quality = Intelligence + Context Quality. </strong> </li>
      </ul>
      <p>So, what happens if LLMs stop getting smarter? The way to win at response quality will be to have the highest <em>context quality</em>. The current battle for the most <em>intelligent</em> LLM will evolve into a battle for the <em>highest quality</em> LLM; i.e. the LLM with the best intelligence + context quality.</p>
      <p>AI products downstream of OpenAI already know this— when everyone has the same intelligence, the battle is about who can do the best <a href="https://blog.langchain.com/the-rise-of-context-engineering/">context engineering</a>. But mainstream consumer AI companies (eg ChatGPT) have focused their investment on maxing out intelligence first.</p>

      <h3>Improving context quality would be a really good thing for consumer AI</h3>


      <p>Most AI interactions today start with you explaining everything.<sup><a href="#fn1" id="fnref1">[1]</a></sup> This is annoying, especially if you've already explained everything somewhere else.</p>
      <p>This information usually isn’t a secret. What model car you drive, the link to the repo you’re working on, the names of your friends: all this info is easy to find from your search history, email, personal notes, etc. </p> 
      <p>AI interactions would be so much easier if they could come pre-loaded with context from these (easily accessible) sources. But that's not how the way things work right now.</p>
      

      <h3>Who controls the personal context?</h3>
      <p>Who has access to the personal context dataset? Not model providers, for the most part. It’s highly balkanized:</p>
      <ul>
        <li>notes you scribble to yourself</li>
        <li>browsing and search history</li>
        <li>content you watch on tiktok and other platforms</li>
        <li>discord servers and group chats</li>
        <li>calendars, emails, files, code repos, etc</li>
      </ul>

      <h3>We’ve seen this movie before</h3>
      <p>AI interactions will personalize at an industrial scale in the 2020s, similarly to content + ads in the 2010s.</p>
      <p>Last decade, we built out massive infrastructure to enable personalized advertising that pulls information from every corner of a user’s online footprint.<sup><a href="#fn2" id="fnref2">[2]</a></sup> We will need to execute a similar-scale infrastructure buildout in order to win the war for best context quality.</p>

      <h3>A reliable way to hydrate AI interactions with high-quality context</h3>


      <h3>Beyond on-demand chat</h3>
      <p>In five years, we'll think it's crazy that we used to have to start every AI interaction by explaining everything. In ten years, we'll think it's crazy that we ever had to "prompt" AI <em>at all</em>. With high-enough-quality context, AI should be able to answer your question before you even ask it.</p> Like personalized ads today, in the future AI will be able to <u>read your mind</u>.
      <ul>
        <li>If it can see you’re stuck on a bug, it texts you what you’re missing.</li>
        <li>If you’re booking a vacation, it does deep research in the background to pre-empt your questions.</li>
      </ul>
      <p>As compute costs decrease (if models can’t get more intelligent, then costs have to go down), we’ll be able to invest more in <a href= https://arxiv.org/abs/2504.13171>Sleep Time Compute</a> to create these pre-empting, magical interactions.</p>

      <h3>Who’s building this?</h3>
      <p>Me and <a href= https://x.com/samzliu> Sam Liu</a>. We’re building <a href="https://allegory.to/">Allegory</a>, a notes app that lets AI take full advantage of the rich and high-quality context that you generate when you write notes to yourself to improve AI interactions. If you think this sounds cool, <a href= https://calendly.com/htdowling/coffee </a> say hi to us</a>.</p>

      <hr>

      <section id="footnotes">
        <ol>
          <li id="fn1">Paraphrasing <a href=https://www.anthropic.com/news/connectors-directory>Anthropic</a>, who articulated it really well. <a href="#fnref3" aria-label="Back to content">↩</a></li>
          <li id="fn2">Aside: we expected this massive consolidation of information by a few key players (we called it “Big Data”) to fundamentally change the way society functioned, but it turned out to mostly just enable better ads. <a href="#fnref1" aria-label="Back to content">↩</a></li>
        </ol>
      </section>

      <br>
      <br>

      <hr>
      <h3>Postscript: What if models <em>do</em> keep getting smarter?</h3>
      <p>These trends actually hold even in the world where AI does get a lot smarter. (I opted to focus on the "what if ai not smarter" for views, haha). Let's examine the priciples underlying the arguments made in this post and see if they still apply in the world where AI gets much smarter. </p>
      <ul>
        <li><strong>Response Quality = Intelligence + Context Quality.</strong> 
          <ul>
            <li>This will still hold. Incremental gains will be achievable by increasing Intelligence <em>or</em> context quality. </li>
            <li>In fact, the relationship between Intelligence and Context Quality is likely <strong>convex</strong>; i.e. the smarter a model is, the <em>more productive</em> an improvement to Context Quality is. </li>
          </ul>
        </li>
        <li><strong>The most important personal context lies outside of intelligence providers.</strong>
          <ul>
            <li> This is clearly still true; you could argue as AI intelligence improves it gives the winning player a chance at becoming the "front door to the internet", but Google already is yet hasn't devoted any serious effort to this problem since their investment has been in intelligence-maxxing.  </li>
          </ul>
        </li>
        <li><strong>AI model costs will go down.</strong>
          <ul>
            <li>
              This may not hold true for all applications, but I suspect it *will* for consumer AI. 
            </li>
            <li>
              We need to ask ourselves: is intelligence the bottleneck for any consumer AI use cases currently?
            </li>
            <li>
              For the largest consumer AI use cases (search, AI companion, cheating on homework), AI already works great. In fact, people complained when OpenAI <em>upgraded</em> ChatGPT to use more intelligent GPT-5 because it had fewer of the sycophantic qualities of 4o.
            </li>
          </ul>
        </li>
        
      </ul>

      

    </main>
  </body>
  </html>


