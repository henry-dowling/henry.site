<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Autocomplete everywhere that reads your mind – Henry Dowling</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" type="image/jpeg" href="favicon.jpg">
  <style>
    body {
      line-height: 1.4;
      font-size: 16px;
      padding: 0 10px;
      margin: 50px auto;
      max-width: 650px;
    }
    #maincontent {
      max-width: 42em;
      margin: 15 auto;
    }
    details {
      margin: 1em 0;
    }
    summary {
      font-weight: 700;
      font-size: 1.17em; /* similar to h3 */
      cursor: pointer;
    }
    figure {
      margin: 0;
    }
    figcaption {
      color: #555;
      font-size: 0.9em;
      text-align: center;
      margin-top: 4px;
    }
  </style>
</head>
<body>
  <header>
    <h2><a href="index.html" style="color: inherit; text-decoration: none;">Henry Dowling</a>'s <a href="blog.html">Blog</a></h2>
  </header>
  <main>
    <h2>Autocomplete everywhere that reads your mind</h2>
    <p style="color:#666; margin-top:-10px;">September 26, 2025</p>

    <img src="images/header-2.png" alt="Codeswitch autocomplete system overview" style="max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px; margin: 10px 0;">

    <p>To explore uses for our personal AI memory store at <a href=https://ferganalabs.com/>Fergana Labs</a>, we built a system-wide contextual autocomplete for MacOS. To hydrate it with context, we built a tool to grab text from input fields via a11y APIs, screen OCR and in some cases a Chrome extension. We named it Codeswitch, and you can try it <a href=https://www.getcodeswitch.com/>here!</a><sup id="fnref-1"><a href="#fn-1">[1]</a></sup>. For a demo of how it works, see <a href=https://x.com/henrytdowling/status/1971444339168641135>here</a>. </p>

    <p>To build this so that it works well on MacOS, we had to make some interesting / unconventional design decisions. In this blog post, I'll walk through these decisions.</p>

    <h3>Using Accessibility tools to mock grey "ghost" text + replace text</h3>

    <p> Most of the standard autocomplete operations (suggested text, replacing text for "rephrase feature") are not possible without MacOS a11y APIs. </p>

    <figure style="margin: 0;">
      <img src="images/ghosttext.png" alt="Screenshot showing ghost text appearing in a text field" style="max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px; margin: 10px 0;">
      <figcaption>We create ghost text <em>EVERYWHERE</em> that a user can type</figcaption>
    </figure>
    
    <ul>
      <li>We generate "ghost text" NSPanel by getting the element and cursor's caret position via <a href=https://developer.apple.com/documentation/applicationservices/axuielement>AXUIElement</a> Position Attributes</li>
      
      <li>We also get the bounds of the Panel with <code>AXUIElementCopyParameterizedAttributeValue</code>, which allows us to render a panel that matches the size of the input field that the cursor is currently in.</li>
      
      <li>For our "rephrase" feature, to insert a rephrase suggestion, we actually manually highlight text and then overwrite the text by copy-pasting, which yields a surprisingly smooth UX.</li>

      <li> Our liberal use of a11y APIs unfortunately means that Codeswitch can never live on the app store. But that's fine, we can self-sign and put it on a <a href=https://www.getcodeswitch.com/>website</a>.</li>
    </ul>

    <h3>Hijacking the system clipboard to actually add the suggestion on "tab"</h3>

    <ul>
      <l> To add text on "tab", we manually append the text to the user's system clipboard, and then mock a cmd-v event to paste it.</li>
      
      <li>To ensure that the user doesn't lose the text that they just copied to overwriting, we also implemented a virtual clipboard similar to the Raycast clipboard history extension.</li>
      
      <li>To handle the case where text is added to the clipboard without CMD-C (for example, a button to click to copy in a browser), we regularly poll for clipboard changes to catch these events and add them to the user's virtual clipboard.</li>
      
      <li>We handle the "tab" shortcut, as well as intercepting CMD-C and CMD-X events, with Global keyboard shortcut interception via <code>NSEvent.addGlobalMonitorForEvents(matching:handler:)</code> for a number of shortcuts, including tab.</li>
    </ul>

    <h3>Reverse engineering Google Docs' canvas-based rendering</h3>

    <p>It was important to us that the project works in Google Docs. However, Google Docs’ canvas-based rendering made all of our techniques for getting text (accessibility selection to get cursor position for grey text, read text, select text to paste over it) useless. As a workaround, we wrote a chrome extension to help us get this information from the user. Unfortunately, that means users need to install the chrome extension for Codeswitch to work in Docs.</p>

    <p>Then we had to figure out what text was selected. Since Google Docs doesn’t actually display any text elements in the browser, we reverse-engineered Google Docs’ API to grab the UI elements that store text within their custom rendering.</p>
    <ul>
      <li>To get cursor, we select for <code>.kix-cursor-karet</code> in the DOM</li>
      <li>To get text, we select <code>.kix-canvas-tile-selection &gt; svg &gt; rect</code></li>
    </ul>

    <img src="images/findingthedoc.png" alt="Reverse-engineering approach to finding text elements in Google Docs" style="max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px; margin: 10px 0;">

    <p>We found <a href=https://github.com/swoorpious/web_server_cpp/issues/1>this</a> discussion of how Google’s Canvas-based rendering works extremely helpful.</p>
    
    <h4>Determining selected text in Google Docs</h4>
    <p>Unfortunately, Google Docs renders each line in the canvas as a monolith, so we still needed to figure out <em>what text was being selected</em>. So, given a cursor position in a line and a set of text in the line, we need to estimate which text was selected. To achieve this, we render actual text invisibly on the page, and then measure at which character the selection must start given its real position.</p>

    <figure style="margin: 0;">
      <img src="images/gdh2.png" alt="Demonstration of Google Docs canvas rendering hack" style="max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px; margin: 10px 0;">
      <figcaption>We determine which characters were highlighted by rendering invisible text in the document with our chrome extension and measuring its length.</figcaption>
    </figure>

    <h3>Screen OCR for context when a11y APIs fail</h3>

    <p>While accessibility APIs are great when they work, plenty of applications (most notably Chrome) don't properly expose their text content through these channels. We built an OCR system to capture context from these stubborn apps<sup id="fnref-2"><a href="#fn-2">[2]</a></sup>.</p>

    <ul>
      <li>We used Apple's Vision framework (<code>VNRecognizeTextRequest</code>) to perform real-time OCR on screen regions around the cursor. The Vision API is surprisingly fast - we could OCR a focused region in ~50ms on an M3 Mac.</li>

      <li>The tricky part was figuring out <em>when</em> to OCR, and <when> to send OCR results to our AI memory backend. We couldn't just OCR the entire screen constantly, due to battery life concerns. To avoid OCRing the same content repeatedly, we implemented a simple cache keyed on window ID + focused element. If the user hadn't switched focus, we'd reuse the previous OCR result.</li>

      <li> Another costly operation was adding our OCR results to the user's AI memory store. To limit waste, we used an edit-distance based heuristic and only sent over a new OCR when text was significantly different, either because the user switched application views or they edited text manually. <sup id="fnref-3"><a href="#fn-3">[3]</a></sup>

      <li>Performance was... acceptable. The OCR itself was fast, but the overhead of screenshot capture + processing meant we had to be conservative about when to trigger it. We ended up disabling it by default because the context quality wasn't worth the battery drain for most users-- not to mention privacy concerns.</li>
    </ul>

    <h2>Learnings</h2>
    <ul>
      <li> AI autocomplete with a memory store is *really good* when you're in a mode of work that involves a lot of "copy-pasting" / sending rapidfire emails / slack messages. Here's a scenario where Codeswitch absolutely kills it: your boss asks you to move a meeting, and you need to go and send an email to 5 separate people to get their okay / buy-in.</li>
      <li> Our reverse-engineering approach to Google Docs really only worked because of how fast coding agents can work. With good AI coding agents, we were able to test many promising reverse-engineering approaches in parallel, which allowed us to find a winning strategy in a reasonable amount of time.</li>
      <li> We built this in Swift to take advantage of accessibility APIs. Swift is a nice language with some cool features but XCode is horribly outdated and slow to build in, especially in the AI coding agents era. In hindsight, we should have used <a href=https://tauri.app/>Tauri</a>. Thanks to <a href=https://charlieholtz.com/> Charlie Holtz</a> for the suggestion!</li>
    </ul>

    <br>

    <em> Shameless plug: If you think this kind of thing is cool, come work with us at <a href=https://ferganalabs.com/>Fergana Labs</a>.</em>
  
    <p id="fn-1"><small>[1] If you're looking for an invite code and don't have one, you may be able to find one if you're clever enough ;)</small></p>
    <p id="fn-2"><small>[2] The OCR feature is currently disabled by default-- we were concerned with privacy and battery life-- but it can be enabled in app settings if you're brave enough to try it out.</small></p>
    <p id="fn-3"><small>[3] We also have additional caching in our AI memory store to limit redundancies, but that's for another blog post.</small></p>
    <p id="fn-3"><small>[4] Related work: check out <a href=https://x.com/oshaikh13/status/1967626897837494479>Tabracadabra</a>, which launched around the same time we built this. When we saw this launch midway through work on this, we concluded that this is an idea that's in the <a href=https://garfield.library.upenn.edu/essays/v4p660y1979-80.pdf?utm_source=chatgpt.com>scientific zeitgeist</a> rn. </small></p>
  </main>
</body>
</html>
